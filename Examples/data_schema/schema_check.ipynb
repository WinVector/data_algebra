{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas data frame is probably the most popular tool used to model tabular data in Python. For in-memory data, Pandas serves a role that might normally fall to a relational database. Though Pandas data frames are typically manipulated through methods, instead of with a relational query language. One can even extend Pandas to accept query languages or operator algebras, as we have done in with [the data algebra](https://github.com/WinVector/data_algebra).\n",
    "\n",
    "However, a common missing component remains: a general \"Pythonic\" [data schema](https://en.wikipedia.org/wiki/Database_schema) definition, documentation, and invariant enforcement mechanism.\n",
    "\n",
    "It turns out it is quite simple to add such functionality using Python decorators. This isn't particularly useful for general functions (such as `pd.merge()`), where the function is supposed to support arbitrary data schemas. However, it can be *very* useful in adding checks and safety to specific applications and analysis workflows built on top such generic functions. In fact, it is a good way to copy schema details from external data sources such as databases or CSV into enforced application invariants. Application code that transforms fixed tables into expected exported results can benefit greatly from schema documentation and enforcement.\n",
    "\n",
    "I propose the following simple check criteria for both function signatures and data frames that applies to both inputs and outputs:\n",
    "\n",
    "  * Data must have *at least* the set of argument names or column names specified.\n",
    "  * Each column must have *no more* types (for non-null values) than the types specified.\n",
    "\n",
    "In this note I will demonstrate the how to add such schema documentation and enforcement to Python functions working over data frames using Python decorators.\n",
    "\n",
    "Let's import our modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import data_algebra as da\n",
    "from data_algebra.data_schema import SchemaCheckSwitch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I said, we are interested in documenting that the data frames we work with have:\n",
    "\n",
    "  * At least the columns we expect.\n",
    "  * No types we don't expect in those columns.\n",
    "\n",
    "These two covariant constraints are what we need to ensure we can write the operations over columns (which we need to know exist), and to not get unexpected results (from unexpected types). Instead of getting down-stream signalling nor non-signalling errors during column operations, we get useful exceptions on columns and values. This can be particularly useful for data science code near external data sources such as databases or CSV (comma separated value) files. Many of these sources themselves have data schemas and schema documentation that one can copy into the application.\n",
    "\n",
    "We also want to be able to turn enforcement on or off in an entire code base easily. To do this we define a indirect importer called [`schema_check.py`](https://github.com/WinVector/data_algebra/blob/main/Examples/data_schema/schema_check.py).  It's code looks like the following:\n",
    "\n",
    "```\n",
    "   from data_schema import SchemaCheckSwitch\n",
    "   # from data_schema import SchemaMock as SchemaCheck\n",
    "   from data_schema import SchemaRaises as SchemaCheck\n",
    "   SchemaCheckSwitch().on()\n",
    "```\n",
    "\n",
    "Isolating these lines in a shared import lets all other code switch behavior by only editing this file.\n",
    "\n",
    "Let's go ahead and import that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a indirect import, so entire package behavior\n",
    "# can be changed globally all at once\n",
    "import schema_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The usual way to define a function in Python is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard define of a function\n",
    "def fn(a, /, b, *, c, d=None):\n",
    "    \"\"\"doc\"\"\"\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's instead, define the same function including the `SchemaCheck` decoration. The details of this decorator are documented [here](https://github.com/WinVector/Examples/tree/main/arg_types#readme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same function definition, now with schema decorator\n",
    "@schema_check.SchemaCheck({\n",
    "        'a': int, \n",
    "        'b': {int, float}, \n",
    "        'c': {'x': int},\n",
    "        },\n",
    "        return_spec={'z': float})\n",
    "def fn(a, /, b, *, c, d=None):\n",
    "    \"\"\"doc\"\"\"\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decorator defines the types schemas of at least a subset of positional and named arguments. Declarations are either values (converted to Python types), Python types, or sets of types. A special case is dictionaries, which specify a subset of the column structure of function signatures or data frames. \"return_spec\" is reserved to name the return schema of the function.\n",
    "\n",
    "Our decorator documentation declares that `fn()` expects at least:\n",
    "\n",
    "  * an argument `a` of type `int`.\n",
    "  * an argument `b` of type `int` or `float`.\n",
    "  * an argument `c` that is a data frame (implied by the dictionary argument), and that data frame contains a column `x` that has no non-null elements of type other than `int`.\n",
    "  * to return a data frame (indicated by the dictionary argument) that has at least a column `z` that contains no non-null elements of type other than `float`.\n",
    "\n",
    "This gives us some enforceable invariants that can improve our code.\n",
    "\n",
    "We can see this repeated back in the decorator altered `help()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show altered help text\n",
    "help(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a learnable schema specification convention.\n",
    "\n",
    "Let's see it catch an error. We show what happens if we call `fn()` with none of the expected arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch schema mismatch\n",
    "threw = False\n",
    "try:\n",
    "    fn()\n",
    "except TypeError as e:\n",
    "    print(e)\n",
    "    threw = True\n",
    "assert threw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, and this is where we start to get benefits, we can call with a wrong argument type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch schema mismatch\n",
    "threw = False\n",
    "try:\n",
    "    fn(1, 2, c=3)\n",
    "except TypeError as e:\n",
    "    print(e)\n",
    "    threw = True\n",
    "assert threw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we show that this checking pushes down into the structure of data frame arguments! In our next example we see the argument is missing a required column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch schema mismatch\n",
    "threw = False\n",
    "try:\n",
    "    fn(1, 2, c=pd.DataFrame({'z': [7]}))\n",
    "except TypeError as e:\n",
    "    print(e)\n",
    "    threw = True\n",
    "assert threw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check column and cell types in addition to mere column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch schema mismatch\n",
    "threw = False\n",
    "try:\n",
    "    fn(1, 2, c=pd.DataFrame({'x': [3.0]}))\n",
    "except TypeError as e:\n",
    "    print(e)\n",
    "    threw = True\n",
    "assert threw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we can check return types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch schema mismatch\n",
    "rv = None\n",
    "threw = False\n",
    "try:\n",
    "    fn(\n",
    "        1, \n",
    "        2, \n",
    "        c=pd.DataFrame({'x': [30], \"z\": [17.2]}), \n",
    "        d=pd.DataFrame({'q': [7.0]}))\n",
    "except TypeError as e:\n",
    "    print(e.args[0])\n",
    "    rv = e.args[1]\n",
    "    threw = True\n",
    "assert threw\n",
    "\n",
    "# the return value is available for inspection\n",
    "rv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the rejected return value is attached to the `TypeError` to help with diagnosis and debugging.\n",
    "\n",
    "Again, these sort of checks are not for generic utility methods (such as `pd.merge()`), which are designed to work over a larger variety of schemas. However, they are very useful near client interfaces, APIs, and database tables. This technique and [data algebra](https://github.com/WinVector/data_algebra) processing may naturally live near data sources. There is a an-under appreciated design principle that package code should be generic, and application code should be specific (even in the same project).\n",
    "\n",
    "Let's show a successful call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn(\n",
    "    1, \n",
    "    b=2, \n",
    "    c=pd.DataFrame({'x': [3]}), \n",
    "    d=pd.DataFrame({'z': [7.0]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn off the checking with a single global command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off checking globally\n",
    "SchemaCheckSwitch().off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice a previously failing call is no longer checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show wrong return value is now allowed\n",
    "fn(\n",
    "    1, \n",
    "    2, \n",
    "    c=pd.DataFrame({'x': [30], \"z\": [17.2]}), \n",
    "    d=pd.DataFrame({'q': [7.0]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value has is missing the required `z` column, but with checks off the function is not interfered with.\n",
    "\n",
    "When checks are on: failures are detected much closer to causes, making debugging and diagnosis much easier. Also, the decorations are a easy way to document in human readable form some basics of the expected input and output schemas.\n",
    "\n",
    "And, the input and output schema are attached to the function as objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show argument schema specifications\n",
    "pprint(fn.data_schema.arg_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show return value schema\n",
    "pprint(fn.data_schema.return_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes the schema data available for other uses.\n",
    "\n",
    "A downside is, the technique *can* run into what I call \"the first rule of meta-programming\". Meta-programming only works as long as it doesn't run into other meta-programming (also called the \"its only funny when I do it\" theorem). That being said, I feel these decorators can be very valuable in Python data science projects.\n",
    "\n",
    "This documentation and demo can be found [here](https://github.com/WinVector/data_algebra/tree/main/Examples/data_schema).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system also works with Polars data frames instead of Pandas as the data frame realization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn back on checking globally\n",
    "SchemaCheckSwitch().on()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failing example in Polars\n",
    "threw = False\n",
    "try:\n",
    "    fn(1, 2, c=pl.DataFrame({'z': [7]}))\n",
    "except TypeError as e:\n",
    "    print(e)\n",
    "    threw = True\n",
    "assert threw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failing example in Polars\n",
    "rv = None\n",
    "threw = False\n",
    "try:\n",
    "    fn(\n",
    "        1, \n",
    "        2, \n",
    "        c=pl.DataFrame({'x': [30], \"z\": [17.2]}), \n",
    "        d=pl.DataFrame({'q': [7.0]}))\n",
    "except TypeError as e:\n",
    "    print(e.args[0])\n",
    "    rv = e.args[1]\n",
    "    threw = True\n",
    "assert threw\n",
    "\n",
    "# the return value is available for inspection\n",
    "rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good example in Polars\n",
    "fn(\n",
    "    1, \n",
    "    b=2, \n",
    "    c=pl.DataFrame({'x': [3]}), \n",
    "    d=pl.DataFrame({'z': [7.0]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we also have simple \"types in data frame\" inspection tools [here](https://github.com/WinVector/data_algebra/blob/main/Examples/data_schema/df_types.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion: the `SchemaCheck` decoration is simple and effective tool to add schema documentation and enforcement to your analytics projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some relevant versions\n",
    "pprint({\n",
    "    'pd': pd.__version__, \n",
    "    'pl': pl.__version__, \n",
    "    'np': np.__version__, \n",
    "    'da': da.__version__})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
