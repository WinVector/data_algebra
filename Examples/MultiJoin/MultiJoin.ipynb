{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# How to Re-Map Many Columns in a Database\n",
    "\n",
    "John Mount, Win-Vector LLC, 2022-01-09\n",
    "https://github.com/WinVector/data_algebra/blob/main/Examples/MultiJoin/MultiJoin.ipynb\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A surprisingly tricky problem in doing data science or analytics in the database are situations where one has to re-map a *large* number of columns. This occurs, for example, in the [vtreat](https://github.com/WinVector/pyvtreat) data preparation system. In the vtreat case, a large number of the variable encodings reduce to table-lookup or re-mapping.\n",
    "\n",
    "For imperative systems, and in-memory data frames (such as Pandas) this presents no great problem (for example pandas.Series.map can be used on each column in sequence).\n",
    "\n",
    "In relational databases the common solutions include:\n",
    "\n",
    "  * Large CASE/WHEN statements (one case per variable level or value).\n",
    "  * Deeply nested JOINs (one per variable).\n",
    "  * Sequenced UPDATE JOINs (one per variable).\n",
    "\n",
    "The tricky part is: data science application scale easily has hundreds of string valued variables, each having hundreds of thousands of tracked values. The possibility of a large number of variable values or level renders the CASE/WHEN solution undesirable- as the query size is proportional to the number variables *and* values. The JOIN solutions build a query size proportional to the number of variables (again undesirable, but tolerable). However, super deeply nested queries are just not what relational databases expect. And a sequence of updates isn't easy to support as a single query or view.\n",
    "\n",
    "The impedance mis-match is: re-mapping a large number of columns is a reasonable ask, but doesn't always result in what is considered a polite query in the database.\n",
    "\n",
    "Thankfully there is at least one more avenue for solution: the relational database's ability to efficiently dispatch operations over a very large number of rows. If we exploit that we can get the *effect* of a large number of mappings in a limited, though by no means small, query.\n",
    "\n",
    "Let's work this as an example.\n",
    "\n",
    "## Our Example\n",
    "\n",
    "First we import our packages, and make our notional example data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_algebra.data_ops import descr\n",
    "from data_algebra.solutions import def_multi_column_map\n",
    "import data_algebra.cdata\n",
    "import data_algebra.test_util\n",
    "import data_algebra.BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "   id va vb\n0   1  a  e\n1   2  b  e\n2   3  a  g\n3   4  c  f",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>va</th>\n      <th>vb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>a</td>\n      <td>e</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>b</td>\n      <td>e</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>a</td>\n      <td>g</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>c</td>\n      <td>f</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'va': ['a', 'b', 'a', 'c'],\n",
    "    'vb': ['e', 'e', 'g', 'f'],\n",
    "})\n",
    "\n",
    "d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For our problem, let's say we want to re-map the values (or levels) seen for the columns `va` and `vb` to numbers. In practice, we may have hundreds of variables, and hundreds and thousands of levels. (We could generalize to sets of columns mapping to sets of columns, but the one to one example is clearer and more common in practice).\n",
    "\n",
    "## Mappings as Data\n",
    "\n",
    "In all case this sort of re-mapping can itself be written as a table."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "  column_name column_value  mapped_value\n0          va            a           1.0\n1          va            b           2.0\n2          vb            e           3.0\n3          vb            f           4.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>column_name</th>\n      <th>column_value</th>\n      <th>mapped_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>va</td>\n      <td>a</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>va</td>\n      <td>b</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>vb</td>\n      <td>e</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>vb</td>\n      <td>f</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = pd.DataFrame({\n",
    "    'column_name': ['va', 'va', 'vb', 'vb'],\n",
    "    'column_value': ['a', 'b', 'e', 'f'],\n",
    "    'mapped_value': [1., 2., 3., 4.],\n",
    "})\n",
    "\n",
    "m"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above mapping table `m` is keyed by `column_name` and `column_value`. A row with a given pair of keys has a payload of `mapped_value` saying what number this combination is mapped to.  We have co-mingled mapping rules for `va` with mapping rules for `vb` into a single table by making sure we have sufficient keys to separate the cases.\n",
    "\n",
    "\n",
    "## Many Mappings in a Single Join\n",
    "\n",
    "If we re-structure the data we can re-write many mappings as a single shared join.\n",
    "\n",
    "What we want is a copy of the data `d` where each row in `d` is represented by multiple rows. This is exactly what databases call an [un-pivot](https://en.wikipedia.org/wiki/Pivot_table#History) or melt, and what the [data algebra cdata system](https://github.com/WinVector/data_algebra/blob/main/Examples/cdata/cdata.ipynb) was designed to manage.\n",
    "\n",
    "Such a transform is specified as follows."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "unpivot = data_algebra.cdata.unpivot_specification(\n",
    "    row_keys=['id'],\n",
    "    col_name_key='column_name',\n",
    "    col_value_key='column_value',\n",
    "    value_cols=['va', 'vb'],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All we have done is say what columns identify records (`id`), which columns we want to take values from (`va` and `vb`), and how we want those values laid-out after the unpivot (by `column_name` and `column_value`).  The effect of the transform is, it re-encodes rows of the data frame as identifiable groups of rows."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "   id column_name column_value\n0   1          va            a\n1   1          vb            e\n2   2          va            b\n3   2          vb            e\n4   3          va            a\n5   3          vb            g\n6   4          va            c\n7   4          vb            f",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>column_name</th>\n      <th>column_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>va</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>vb</td>\n      <td>e</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>va</td>\n      <td>b</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>vb</td>\n      <td>e</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>va</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3</td>\n      <td>vb</td>\n      <td>g</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4</td>\n      <td>va</td>\n      <td>c</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>4</td>\n      <td>vb</td>\n      <td>f</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpivot.transform(d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice each row `id` now occurs twice. The important observation is: in remapping all of our variables is just a *single* join against `m` using `column_name` and `column_value` as composite join keys. The entire transform, join, and then inverse transform (back into all entries in single row format: a fundamental form we call a \"row record\")."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "ops = def_multi_column_map(\n",
    "    descr(d=d),\n",
    "    mapping_table=descr(m=m),\n",
    "    row_keys=['id'],\n",
    "    cols_to_map=['va', 'vb'],\n",
    "    coalesce_value=0.0,\n",
    "    cols_to_map_back=['va_mapped', 'vb_mapped'],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We expect `ops` to re-process `d` into the following."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "   id  va_mapped  vb_mapped\n0   1        1.0        3.0\n1   2        2.0        3.0\n2   3        1.0        0.0\n3   4        0.0        4.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>va_mapped</th>\n      <th>vb_mapped</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.0</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expect = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'va_mapped': [1.0, 2.0, 1.0, 0.0],\n",
    "    'vb_mapped': [3.0, 3.0, 0.0, 4.0],\n",
    "})\n",
    "\n",
    "expect"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see if we get that result."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "   id  va_mapped  vb_mapped\n0   1        1.0        3.0\n1   2        2.0        3.0\n2   3        1.0        0.0\n3   4        0.0        4.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>va_mapped</th>\n      <th>vb_mapped</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.0</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = ops.eval({'d': d, 'm': m})\n",
    "\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we can confirm the results do indeed match."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "assert data_algebra.test_util.equivalent_frames(res, expect)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## In Database\n",
    "\n",
    "`ops` is a [data algebra](https://github.com/WinVector/data_algebra) pipeline (actually a directed acyclic graph, or DAG). Being such, it also can be run on adapted database by automatic translation to SQL.\n",
    "\n",
    "Let's see this run in the Google BigQuery database.\n",
    "\n",
    "First we build our connection, and insert our example data. Of course, the point of using a database is that the data is usually *already* there (not something we insert)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# try it in database\n",
    "db_handle = data_algebra.BigQuery.example_handle()\n",
    "db_handle.insert_table(d, table_name='d', allow_overwrite=True)\n",
    "db_handle.insert_table(m, table_name='m', allow_overwrite=True)\n",
    "db_handle.drop_table(\"merged\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then create a table of results, without any additional data motion to or from the database."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "db_handle.execute(\n",
    "    f\"CREATE TABLE {db_handle.db_model.table_prefix}.merged AS {db_handle.to_sql(ops)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To look at the result, we bring it back to Python/Jupyter."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "   va_mapped  vb_mapped  id\n0        1.0        3.0   1\n1        2.0        3.0   2\n2        1.0        0.0   3\n3        0.0        4.0   4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>va_mapped</th>\n      <th>vb_mapped</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_res = db_handle.read_query(\n",
    "    f\"SELECT * FROM {db_handle.db_model.table_prefix}.merged ORDER BY id\")\n",
    "\n",
    "db_res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "assert data_algebra.test_util.equivalent_frames(db_res, expect)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we have confirmed, we get the same result. This sort of methodology is what allows the [vtreat](https://github.com/WinVector/pyvtreat) data preparation system to be run in a database at data warehouse scale.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We showed how to efficiently perform a large number of variable re-mappings in a SQL database. Variable re-mapping or lookup is a fundamental step for data analytics and data science. We can translate the process to SQL's primary operation: the JOIN. Some care must be taken to translate into a *small* number of operations over a *large* number of rows, as this is where relational databases shine."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Appendix\n",
    "\n",
    "### The Data Algebra Pipeline\n",
    "\n",
    "We can take a look at the actual data algebra pipeline. It is, as promised, converting the records, doing one big join, and then converting the records back. Though it is nice to have a convenience function to write out such a pipeline for us (the huge advantage of programming over composable, inspectable, and value oriented APIs)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(\n    TableDescription(table_name=\"d\", column_names=[\"id\", \"va\", \"vb\"])\n    .select_columns([\"id\", \"va\", \"vb\"])\n    .convert_records(\n        data_algebra.cdata.RecordMap(\n            blocks_in=None,\n            blocks_out=data_algebra.cdata.RecordSpecification(\n                record_keys=[\"id\"],\n                control_table=pd.DataFrame(\n                    {\"column_name\": [\"va\", \"vb\"], \"column_value\": [\"va\", \"vb\"],}\n                ),\n                control_table_keys=[\"column_name\"],\n            ),\n        )\n    )\n    .natural_join(\n        b=TableDescription(\n            table_name=\"m\", column_names=[\"column_name\", \"column_value\", \"mapped_value\"]\n        ).select_columns([\"column_name\", \"column_value\", \"mapped_value\"]),\n        by=[\"column_name\", \"column_value\"],\n        jointype=\"LEFT\",\n    )\n    .extend({\"mapped_value\": \"mapped_value.coalesce(0.0)\"})\n    .convert_records(\n        data_algebra.cdata.RecordMap(\n            blocks_in=data_algebra.cdata.RecordSpecification(\n                record_keys=[\"id\"],\n                control_table=pd.DataFrame(\n                    {\"column_name\": [\"va\", \"vb\"], \"mapped_value\": [\"va\", \"vb\"],}\n                ),\n                control_table_keys=[\"column_name\"],\n            ),\n            blocks_out=None,\n        )\n    )\n    .rename_columns({\"va_mapped\": \"va\", \"vb_mapped\": \"vb\"})\n)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The SQL\n",
    "\n",
    "We can also take a peek at the SQL realizing this pipeline over Google BigQuery. It is large, because the record transformation steps are themselves realized in terms of SQL primitives (unpivots are joins, and pivots are aggregations).  The main thing we can say about this query is, we didn't have to write it! And yes, it is machine generated SQL deliberately targeting a simple sub-grammar of the language. So hand-rolled SQL would be smaller."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- data_algebra SQL https://github.com/WinVector/data_algebra\n",
      "--  dialect: BigQueryModel 1.4.1\n",
      "--       string quote: \"\n",
      "--   identifier quote: `\n",
      "WITH\n",
      " `convert_records_blocks_out_1` AS (\n",
      "  -- convert records blocks out\n",
      "  SELECT\n",
      "     a.`id` AS `id`,\n",
      "     b.`column_name` AS `column_name`,\n",
      "     CASE   WHEN CAST(b.`column_value` AS STRING) = \"va\" THEN a.`va`   WHEN CAST(b.`column_value` AS STRING) = \"vb\" THEN a.`vb`  ELSE NULL END AS `column_value`\n",
      "   FROM ( SELECT * FROM\n",
      "   `data-algebra-test.test_1.d`\n",
      "    ) a\n",
      "   CROSS JOIN (\n",
      "    SELECT\n",
      "     *\n",
      "    FROM (\n",
      "        (SELECT \"va\" AS `column_name`, \"va\" AS `column_value`)\n",
      "        UNION ALL (SELECT \"vb\" AS `column_name`, \"vb\" AS `column_value`)\n",
      "    ) `table_values`\n",
      "    ) b\n",
      "    ORDER BY\n",
      "    a.`id`,\n",
      "    b.`column_name`\n",
      " ) ,\n",
      " `natural_join_0` AS (\n",
      "  SELECT  -- _0..natural_join(b= _1, by=['column_name', 'column_value'], jointype='LEFT')\n",
      "   `id` ,\n",
      "   COALESCE(`join_source_left_0`.`column_name`, `join_source_right_0`.`column_name`) AS `column_name` ,\n",
      "   COALESCE(`join_source_left_0`.`column_value`, `join_source_right_0`.`column_value`) AS `column_value` ,\n",
      "   `mapped_value`\n",
      "  FROM\n",
      "  (\n",
      "   `convert_records_blocks_out_1` `join_source_left_0`\n",
      "  LEFT JOIN\n",
      "   `data-algebra-test.test_1.m` `join_source_right_0`\n",
      "  ON (\n",
      "   `join_source_left_0`.`column_name` = `join_source_right_0`.`column_name`  AND\n",
      "   `join_source_left_0`.`column_value` = `join_source_right_0`.`column_value`\n",
      "  )\n",
      "  )\n",
      " ) ,\n",
      " `extend_2` AS (\n",
      "  SELECT  -- .extend({ 'mapped_value': 'mapped_value.coalesce(0.0)'})\n",
      "   `id` ,\n",
      "   `column_name` ,\n",
      "   `column_value` ,\n",
      "   COALESCE(`mapped_value`, 0.0) AS `mapped_value`\n",
      "  FROM\n",
      "   `natural_join_0`\n",
      " ) ,\n",
      " `convert_records_blocks_in_3` AS (\n",
      "  -- convert records blocks in\n",
      "  SELECT\n",
      "     `id` AS `id`,\n",
      "     MAX(CASE WHEN  ( CAST(`column_name` AS STRING) = \"va\" )  THEN `mapped_value` ELSE NULL END) AS `va`,\n",
      "     MAX(CASE WHEN  ( CAST(`column_name` AS STRING) = \"vb\" )  THEN `mapped_value` ELSE NULL END) AS `vb`\n",
      "   FROM ( SELECT * FROM\n",
      "   `extend_2`\n",
      "    ) a\n",
      "   GROUP BY\n",
      "    `id`\n",
      "   ORDER BY\n",
      "    `id`\n",
      " )\n",
      "SELECT  -- .rename_columns({'va_mapped': 'va', 'vb_mapped': 'vb'})\n",
      " `va` AS `va_mapped` ,\n",
      " `vb` AS `vb_mapped` ,\n",
      " `id`\n",
      "FROM\n",
      " `convert_records_blocks_in_3`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(db_handle.to_sql(ops))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean Up"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "db_handle.drop_table(\"df\")\n",
    "db_handle.drop_table(\"m\")\n",
    "db_handle.drop_table(\"xicor\")\n",
    "db_handle.close()\n",
    "# show we made it to here, and did not assert earlier\n",
    "print('done')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}